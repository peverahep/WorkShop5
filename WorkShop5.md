# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]

Отчет по лабораторной работе #5 выполнил(а):

- Мальцев Богдан Андреевич
- ФО-220005

Отметка о выполнении заданий (заполняется студентом):

| Задание   | Выполнение | Баллы |
| --------- | ---------- | ----- |
| Задание 1 | \*         | 60    |
| Задание 2 | \*         | 20    |
| Задание 3 | \*         | 20    |

знак "\*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:

- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы

познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

Ход работы:

## Задание 1

### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

В машинном обучении коэфициент корреляции-это взаимосвязь переменной-предиктора и целевой переменной (Target Variable). На основе этого определения происходили поиски внутри C# скрипта. В ходе поисков я увидел, что текущую взаимосвязь между Agent и Target выражает переменная "distanceToTarget". В качестве числовой характеристики данной взаимосвязи, определяющей дальнейший ход обучения является коэффициент 1.42

![Alt text](https://sun9-13.userapi.com/impg/H_EM3F13LaxhSRueLnsY_gR2TQkunQxtHe3qfA/im3sHCV9YSg.jpg?size=1031x193&quality=96&sign=9928bb34368d6d8595613d6438fbc070&type=album "Коэффицент корреляции")

В данном случае влияние коэффициента корреляции на обучение следующее: Чем больше данный коэффициент, тем более быстрым, но менее точным будет обучение. При уменьшении данного параметра эффект будет прямо противоположный.

## Задание 2

### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

визуализация динамики процесса обучения в зависимости от изменяемой величины будет произведена посредством TensorBoard. Для каждой попытке обучения будет отводиться 100 тысяч шагов, как оптимальное значение.

1. extrinsic -> strength - коэффициент кратности вознаграждения окружающей среды. Чем выше данное значение, тем выше вознаграждение окружения агента

Значение 0.25:

![Alt text](https://sun9-67.userapi.com/impg/Cwcpsvk2mRQMF98jp3GJAxjE1gdxuj5CqQeJzw/OZutkyA-a6Y.jpg?size=840x805&quality=96&sign=1e8facbf9f883d9e0a15f0d4ca89a1e1&type=album "Environment, extrinsic -> strength = 0.25")

![Alt text](https://sun9-52.userapi.com/impg/0yEddNJCVFohgQYX0gwNOVA94ORADEqQSyJPqA/9x49yj2zpKc.jpg?size=1211x719&quality=96&sign=9af908ab3b62cf1d4c8de538fd4e6574&type=album "Policy, extrinsic -> strength = 0.25")

---

Значение 10.0:

![Alt text](https://sun9-78.userapi.com/impg/Oaa8Ds1mHFC_VU_rCpjwbsxuvnTiBU8bI0OjQA/_zUNwounJDo.jpg?size=840x791&quality=96&sign=e645c5be802ab4831d0802fd3dd3693a&type=album "Environment, extrinsic -> strength = 10.0")

![Alt text](https://sun9-45.userapi.com/impg/5QICux2yoNfs4BMC4KVb9QdjbpefLdsARUNieg/d4_-suDiLtI.jpg?size=1199x725&quality=96&sign=039e46a39f3ee433b5cf9d9f29965783&type=album "Policy, extrinsic -> strength = 10.0")

Графики показывают, что при внутреннем вознаграждении =1, вознаграждение окружающей среды равняется коэффициенту кратности, что соответствует ожиданиям. Однако мы можем видеть, что это не единственное влияние данного параметра. График потери значения при высоком значении extrinsic -> strength приобретает скачущую природу, что сигнализирует нам о дестабилизации обучения. Необходимо правильно подбирать значение этого параметра в зависимости от сигнала награды.

2. time_horizon - количество шагов, которое должен сделать агент, прежде чем он получит награду и будет добавлен в буфер опыта, который в дальнейшем будет использоваться для обновления поведения нашей нейронной сети. Чем больше данное значение, тем больше шагов может сделать агент для получения вознаграждения. Слишком большое значение данного параметра может привести к большому количеству лишних действий и дестабилизации обучения, а слишком маленькие значения могут сделать обучение вовсе невозможным, так как агенту будет предоставляться количество шагов меньшее мнимально необходимого числа шагов для достижения цели. Для того параметра необходимо искать золотую середину и подбирать его значение исходя из задачи обучения.

Значение 32:

![Alt text](https://sun9-33.userapi.com/impg/AmXYR4OSsClLcprB_uewJnirPuPYdsqZkO4bkw/XZfiqy0n7nA.jpg?size=835x802&quality=96&sign=89a2c7a84ceea973797bcfea2ca203aa&type=album "Environment, time_horizon = 32")

![Alt text](https://sun3-21.userapi.com/impg/uku__DgpRTMKnKvbgLGI-SDtiULMZhnlT1PfCw/KjYvm1FtcyQ.jpg?size=1199x723&quality=96&sign=49b6f9c22028fa824e46f9c58c3f4efd&type=album "Policy, time_horizon = 32")

---

Значение 4096:

![Alt text](https://sun9-17.userapi.com/impg/mI5XZipTgLlwp3RejEY295ozoB4d-5nEBZuB5A/x6if7xXrXWk.jpg?size=850x809&quality=96&sign=6850d30b9a8869c94a2629e4b08901a1&type=album "Environment, time_horizon = 4096")

![Alt text](https://sun9-65.userapi.com/impg/kQ4q9EcvV5TV_VfTDMk0EijxZLV83k6Ivgbhxw/ZoSuw_DVek8.jpg?size=1187x731&quality=96&sign=30d333b961cd7adcafbcd55e0674344e&type=album "Policy, time_horizon = 4096")

Графики показывают, что при достаточно малом значении данного параметра агент обучается очень стабильно. Это обусловлено простотой задачи, поставленной перед агентом, а именно дойти до шахты и обратно. На данном примере очень хорошо выделяется пагубное влияние слишком высокого значения time_horizon. Как видно из графиков, при значении time_horizon=4096 награда агента приобрела крайне скачущий и откровенно нестабильный вид. Это сигнализирует нам об ухудшении качества обучения.

3. hyperparameters -> epsilon - данный параметр определяет степень возможного изменения политики агента. Маленькие значение пораждают аккуратное, но очень долгое обучение. Высокие же значения напротив, придают обучению более агрессивный и нестабильный характер.

Значение 0.005:

![Alt text](https://sun9-30.userapi.com/impg/b3HpzDZTQ7y19gFc113S4EB2VPPqk2erwZr3Vw/__QDa1CprC8.jpg?size=813x795&quality=96&sign=0b3189743059a890f729b092642fdda9&type=album "Environment, hyperparameters -> epsilon = 0.005")

![Alt text](https://sun9-23.userapi.com/impg/G8C1uXlWKLJb9zGufc0_d72POx3rg4sHPpAnyQ/CxJlpAiDK0o.jpg?size=1197x718&quality=96&sign=06761600a68c1b242a682f7489c170d8&type=album "Policy, hyperparameters -> epsilon = 0.005")

---

Значение 2.5:

![Alt text](https://sun9-29.userapi.com/impg/5KbhVHsG-INxHipZuxvt2vJsNEPN2KBO8xcKNg/kMYNnVn7CMM.jpg?size=856x766&quality=96&sign=74bcc4d6ccf3ae7b787d50aba01b155f&type=album "Environment, hyperparameters -> epsilon = 2.5")

![Alt text](https://sun9-38.userapi.com/impg/rM4QfNT_2JeqllYHgkKOTQWuP3RrPuOzCuaF3w/hHW9QRw9_bk.jpg?size=1187x725&quality=96&sign=23583b88925f135212a6a8251455a512&type=album "Policy, hyperparameters -> epsilon = 2.5")

Графики наглядно показывают, что низкое значение hyperparameters -> epsilon пораждает достаточно стабильный процесс обучения, что видно на графиках награды и ошибок. При этом график "Eplsilon" демонстрирует стремление рассматриваемого значения стать больше, что наглядно показывает, "удушающее" действие низких значений данного параметра.
Высокие же значения вызвали дестабилизацию обучения, это хорошо видно из низких значений графика награды и высоких значений графика ошибок. Также выше-описанные графики приобрели скачущую природу. График "Epsilon", в отличии от первого случая, теперь имеет явную тенденцию на спад, это отражает попытки агента найти более узкую и определённую политику поведения и уменьшить степень изменчивости своих действий.
Не смотря на то, что параметр hyperparameters -> epsilon является в какой-то степени самокалибрующимся, возложение полной ответственности нахождения оптимального значения данного параметра на агента будет являтся ошибкой, которая в лучшем случае повлечёт за собой медленное и нестабильное обучение, а в худшем вовсе может сделать обучение невозможным. В значении hyperparameters -> epsilon необходимо искать золотую середину и подбирать его взависимости от условий обучения.

4. extrinsic -> gamma - коэффициент скидки будущих награждений.
   Этот коэффициент определяет, насколько действия агента будут направлены на получение вознаграждения в отдалённом будущем. Чем больше этот параметр, тем более далёкое планирование будет совершать агент. Данный праметр должен быть строго меньше единицы.

Значение 0.01:

![Alt text](https://sun9-58.userapi.com/impg/ULfv8UXh4YkMLG5t85si6ou8bFwd5j90Livq0g/HrS54sJ9Isk.jpg?size=825x806&quality=96&sign=80a705907c472a53c0079a6c87d304eb&type=album "Environment, extrinsic -> gamma = 0.01")

![Alt text](https://sun9-6.userapi.com/impg/7bHpMRaju6KSWYlw3MwIEw6gIcU5ILjLt1ff1w/6KyAcueR6DQ.jpg?size=1190x717&quality=96&sign=07e9fa29ded877800c9680b2cd4ac63c&type=album "Policy, extrinsic -> gamma = 0.01")

---

Значение 0.99:

![Alt text](https://sun9-25.userapi.com/impg/pW8Db3LeMyL2UQXX-QQ3WDAhen8K3PCY4eQPhg/EDdDyBbq9LE.jpg?size=820x805&quality=96&sign=21d9f4cf1815b8a6109295197454b89e&type=album "Environment, extrinsic -> gamma = 0.99")

![Alt text](https://sun9-68.userapi.com/impg/PwxYlU8LhMN_VUv22gH1c5qZyo95JcgrnenCzw/PA3Oj2pY5s4.jpg?size=1221x722&quality=96&sign=448e9a90e8f0557f4375eb75071d4bc5&type=album "Policy, extrinsic -> gamma = 0.99")

При высоких значениях данного параметра мы можем видеть хоть слегка и запоздавшее (с 20000 шага), но стабильное и высокое значение награды равное 1. Это наглядно показывает работу агента на будущее.
Напротив, маленькое значение вызвало нарастающую тенденцию награды фактически с самого начала обучения, так как агент пытается получить награду здесь и сейчас.
Также видно, что низкое значение данного параметра вызывает блокировку накопления знаний агента (График Value Estimate стремиться к нулю). Это демонстрирует нам незаинтересованность агента в аккумулировании знаний, так как его не интересует награда в будущем.

## Задание 3

### Приведите примеры, для каких игровых задач и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Агент, который следует за определённым объектом может быть полезен в разработке игр, в которых сделана система преследующих питомцев. Также такой агент мог бы быть полезен для игр с механикой погонь в открытом мире.

Агент, который следует по определённому маршруту от точки А к точке Б может быть полезен при разработке игр с открытым миром для игрока, но с закрытым миром для NPC (Например, чтобы охраник не отходил от охроняемого объекта, при этом бы выполнялось полноценное патрулирование). Также такой агент был бы полезен в "Дерби-гонках", чтобы выкинутый с трассы соперник не терялся, а адаптировался к изменяющимся условиям среды.

ML-Agent лучше всего использовать для игр с динамически изменяющимся миром. В таких играх от NPC требуется постоянная адаптация к действиям игрока и изменениям окружающего мира. Также ML-Agent может подойти, если конечный алгоритм программной реализации является трудноподдерживаемой нечитаемой простынёй. Напротив, если задача очень простая, игровой мир является статичным, а все его алгоритмы являются кроткими, эффективными и легко-поддерживаемыми, то в использовании ML-Agent нет смысла и он только за зря нагрузит игру

## Выводы

В ходе лабораторной работы были изучены инструменты для создания системы машинного обучения ML-Agent с последующей интеграцией в Unity. Также была построена визуализация на основе логов обучения средствами TebsorBoard

Цели лабораторной работы были достигнуты.

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
